{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e20321a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-01 15:57:44.022\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdocsumo_image_util.parse.ocr.google\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m47\u001b[0m - \u001b[33m\u001b[1mGOOGLE_APPLICATION_CREDENTIALS was not set. Make sure you have configured Application Default Credentials. \u001b[0m\n",
      "\u001b[32m2025-05-01 15:57:44.023\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdocsumo_image_util.parse.ocr.google\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m50\u001b[0m - \u001b[33m\u001b[1mSet base64 of service account json as GOOGLE_APPLICATION_CREDENTIALS on docker. If local, set above env as json path\u001b[0m\n",
      "\u001b[32m2025-05-01 15:57:44.025\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdocsumo_image_util.parse.ocr.ms\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m29\u001b[0m - \u001b[33m\u001b[1mAzure Subscription key not found...\u001b[0m\n",
      "\u001b[32m2025-05-01 15:57:44.026\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdocsumo_image_util.parse.ocr.ms\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m33\u001b[0m - \u001b[33m\u001b[1mAzure Region not found...\u001b[0m\n",
      "\u001b[32m2025-05-01 15:57:44.027\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdocsumo_image_util.parse.ocr.ms\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m37\u001b[0m - \u001b[33m\u001b[1mAzure Computer Vision Endpoint not found...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from docsumo_image_util.api import parse_pdf\n",
    "import pandas as pd\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "from typing import Union\n",
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List, Tuple\n",
    "from docllm.caller.providers import Provider, ProviderFactory\n",
    "from docllm.config import CallerConfig\n",
    "from docllm.handlers import YamlFileHandler\n",
    "from docllm.parser.input_parser import get_line_text\n",
    "from docllm.parser.output_parser import parse_json\n",
    "from sklearn.metrics import classification_report\n",
    "from docllm.prompt.render import JinjaRenderer\n",
    "from loguru import logger\n",
    "from tenacity import retry, stop_after_attempt\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "# Create a lock for token updates\n",
    "token_lock = threading.Lock()\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/media/veracrypt1/GAC.json\"\n",
    "# from app.config import config_by_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9deb2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifer(ABC):\n",
    "    \"\"\"Base Classifier for Document Splitting.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _classify(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Classifies a list of dataframe to it respective labels\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def classify(self, df: List[Dict]) -> Dict:\n",
    "        \"\"\"Main function where the df from process_image/request is passed into.\n",
    "\n",
    "        Args:\n",
    "            df (List): A list containing dictionary of OCR data\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary with pages, classifications and classified as a key.\n",
    "        \"\"\"\n",
    "        if not df:\n",
    "            print(\"[DEBUG] No data, returning empty.\")\n",
    "            return {\"pages\": [], \"classifications\": [], \"classified\": False}\n",
    "\n",
    "        # # convert dataframe of whole document to page datraframe\n",
    "        # df_pages = pd.DataFrame(df)\n",
    "        # merged_output={}\n",
    "        # strict_batched_df = self.split_strict_df(df_pages)\n",
    "        # for single_batch in strict_batched_df:\n",
    "        #     _pages = list(set(single_batch.page))\n",
    "        #     df_lst = [single_batch[single_batch.page == idx] for idx in _pages]\n",
    "        #     out = self._classify(df_lst)\n",
    "        #     # Initialize keys if not present\n",
    "        #     merged_output.setdefault(\"pages\", []).extend(out.get(\"pages\", []))\n",
    "        #     merged_output.setdefault(\"classifications\", []).extend(out.get(\"classifications\", []))\n",
    "        #     merged_output[\"is classified\"] = merged_output.get(\"is classified\", True) and out.get(\"is classified\", True)\n",
    "          \n",
    "        # return merged_output\n",
    "\n",
    "        # convert dataframe of whole document to page datraframe \n",
    "        df_pages = pd.DataFrame(df)\n",
    "        _pages = list(set(df_pages.page))\n",
    "        logger.info(f\"The pages in the parsed document is {_pages}\")\n",
    "        df_lst = [df_pages[df_pages.page == idx] for idx in _pages]\n",
    "        doc_type_lst, pg_chunk_lst, total_input_tokens, total_output_tokens = self._classify(df_lst)\n",
    "        return doc_type_lst, pg_chunk_lst, total_input_tokens, total_output_tokens\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7280645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_bool(v):\n",
    "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "\n",
    "\n",
    "class DoctypeConfig(BaseModel):\n",
    "    doc_type_id: str = Field(..., description=\"Doctype ID\")\n",
    "    doc_type_title: str = Field(..., description=\"Doctype Title\")\n",
    "    prompt: str = Field(\"\", description=\"Prompt for the doctype\")\n",
    "\n",
    "\n",
    "class RequestConfig(BaseModel):\n",
    "    df: List[Dict] = Field(..., description=\"OCRed Dataframes for the file\")\n",
    "    doc_type_details: List[DoctypeConfig] = Field(..., description=\"Document Type Details\")\n",
    "    auto_classification_prompt: str = Field(\"\", description=\"Prompt setup in auto_classify doctype\")\n",
    "    auto_classify: bool = Field(False, description=\"Auto classify flag\")\n",
    "\n",
    "    @field_validator(\"auto_classify\")\n",
    "    def convert_2_bool(cls, v):\n",
    "        return str_to_bool(str(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "738c2669",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_RES = {\"pages\": [], \"classifications\": [], \"classified\": True}\n",
    "\n",
    "class LLMClassifier(Classifer):\n",
    "    os.environ[\"LLM_PROVIDER\"] = \"openrouter\"\n",
    "    #pass your api_key here\n",
    "    LLM_PROVIDER = ProviderFactory.create_provider(provider_name= os.environ.get(\"LLM_PROVIDER\", \"openai\"), api_key=os.getenv(\"OPENROUTER_API_KEY\"))\n",
    "\n",
    "    def __init__(self, request_config: RequestConfig, model_name:str) -> None:\n",
    "        #self.LLM_PROVIDER=ProviderFactory.create_provider(provider_name= os.environ.get(\"LLM_PROVIDER\", \"openai\"), api_key=os.getenv(\"OPENROUTER_API_KEY\"))\n",
    "        self.request_config = request_config\n",
    "        self.doc_types = [doc_type.doc_type_title for doc_type in request_config.doc_type_details]\n",
    "        self.doc_types_mapper = {\n",
    "            doc_type.doc_type_title: doc_type.doc_type_id for doc_type in request_config.doc_type_details\n",
    "        }\n",
    "        self.model_name = model_name\n",
    "    def build_prompts(\n",
    "        self, text_lst: List[str], product_prompt: str, is_auto_classify: bool, doc_types: Union[str, List[str]]\n",
    "    ) -> Tuple[str, str]:\n",
    "        config_name = \"auto_classify_config\" if is_auto_classify else \"custom_doctype_config\"\n",
    "        config_path = f\"objs/{config_name}.yaml\"\n",
    "        yaml_contents = YamlFileHandler(config_path).handle()\n",
    "        system_prompt = yaml_contents.get(\"system_prompt\", \"\")\n",
    "        user_prompt = yaml_contents.get(\"user_prompt\", \"\")\n",
    "        final_user_prompt = JinjaRenderer().render(\n",
    "            user_prompt,\n",
    "            context={\"ocr_text\": text_lst, \"doc_types\": doc_types, \"prompt\": product_prompt},\n",
    "        )\n",
    "        return system_prompt, final_user_prompt\n",
    "\n",
    "\n",
    "    def call_llm(self, llm_provider: Provider, user_prompt: str, system_prompt: str) -> Dict[str, str]:\n",
    "        res = llm_provider.call(\n",
    "            system_prompt=system_prompt,\n",
    "            user_prompt=user_prompt,\n",
    "            #write your model_name here\n",
    "            caller_config=CallerConfig(model_name = self.model_name, allow_model= True),\n",
    "        )\n",
    "\n",
    "        if isinstance(res, str):\n",
    "            output = res\n",
    "        else:\n",
    "            output = res.text\n",
    "        return parse_json(output) , llm_provider.prompt_tokens, llm_provider.completion_tokens\n",
    "    \n",
    "    def create_chunk(self, classifier_out: List[Tuple]) -> Tuple[List, List]:\n",
    "        \"\"\"Creates chunk based on the classification output. It combines same doctype pages\n",
    "        that are present adjancent to each other.\n",
    "\n",
    "        Args:\n",
    "            classifier_out (List[Tuple]): Each item in the list is the output from the\n",
    "                `app.services.classify.classifier.DocClassifierV2._classify_df`.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List, List]: Tuple containing chunk lists for pages and doc_types.\n",
    "\n",
    "        Example:\n",
    "            Case 1:\n",
    "            _______\n",
    "            If the classifier_out:\n",
    "            [\n",
    "             ('form_1040___start', 1), ('form_1040', 2), ('form_1040___start', 3),\n",
    "             ('form_1040', 4), ('form_8995a___start', 5), ('form_8995a', 6), ('acord25', 7), ('auto_classify', 8),\n",
    "             ('acord25', 9), ('auto_classify', 10)\n",
    "            ]\n",
    "\n",
    "            `create_chunk` output is:\n",
    "            (\n",
    "              [\n",
    "               [1, 2, 3, 4], [5, 6], [7], [8], [9], [10]\n",
    "              ],\n",
    "              [\n",
    "               ['form_1040___start', 'form_1040', 'form_1040___start', 'form_1040'],\n",
    "               ['form_8995a___start', 'form_8995a'],\n",
    "               ['acord25'],\n",
    "               ['auto_classify'],\n",
    "               ['acord25'], ['auto_classify']\n",
    "              ]\n",
    "            )\n",
    "\n",
    "            Case 2:\n",
    "            _______\n",
    "            [\n",
    "             ('form_1040___start', 1), ('form_1040', 2),\n",
    "             ('form_1040___start', 3),('form_1040', 4),\n",
    "             ('form_8995a___start', 5), ('form_8995a', 6),\n",
    "             ('form_1040___start', 7),('form_1040', 8),\n",
    "            ]\n",
    "\n",
    "            Output\n",
    "            _______\n",
    "            (\n",
    "             [[1, 2, 3, 4], [5, 6], [7, 8]],\n",
    "             [['form_1040___start', 'form_1040', 'form_1040___start', 'form_1040'],\n",
    "              ['form_8995a___start', 'form_8995a'],\n",
    "              ['form_1040___start', 'form_1040']\n",
    "             ]\n",
    "            )\n",
    "        \"\"\"\n",
    "        n_pages = len(classifier_out)\n",
    "        last_doc_type = classifier_out[0][0]\n",
    "\n",
    "        page_chunk = []\n",
    "        page_chunk_lst = []\n",
    "        doc_types = []\n",
    "        doc_types_lst = []\n",
    "        start_keyword = \"___start\"\n",
    "        chunked = 0  # flag to make sure if we miss out pages to chunk.\n",
    "\n",
    "        for doc_type, page_num in classifier_out:\n",
    "            if not last_doc_type == doc_type:\n",
    "                if (last_doc_type + start_keyword == doc_type) or (doc_type + start_keyword == last_doc_type):\n",
    "                    page_chunk.append(page_num)\n",
    "                    doc_types.append(doc_type)\n",
    "                    continue\n",
    "                page_chunk = sorted(list(set(page_chunk)))\n",
    "                chunked += len(page_chunk)\n",
    "                page_chunk_lst.append(page_chunk)\n",
    "                doc_types_lst.append(doc_types)\n",
    "                page_chunk = [page_num]\n",
    "                doc_types = [doc_type]\n",
    "            else:\n",
    "                page_chunk.append(page_num)\n",
    "                doc_types.append(doc_type)\n",
    "            last_doc_type = doc_type\n",
    "\n",
    "        # check the flag and append the remaining pages chunks.\n",
    "        if chunked != n_pages and page_chunk:\n",
    "            page_chunk_lst.append(page_chunk)\n",
    "            doc_types_lst.append(doc_types)\n",
    "\n",
    "        # almost a dead code due to the above `chunked` flag\n",
    "        # If all pages is auto_classify/delete or single doc_type\n",
    "        if not page_chunk_lst:\n",
    "            page_chunk_lst.append(list(range(1, n_pages + 1)))\n",
    "            doc_types_lst.append([doc_type] * n_pages)\n",
    "\n",
    "        last_chunk_page = page_chunk_lst[-1][-1]\n",
    "        if last_chunk_page != n_pages:\n",
    "            page_chunk = list(range(last_chunk_page + 1, n_pages + 1))\n",
    "            doc_types_lst.append([classifier_out[-1][0]] * len(page_chunk))\n",
    "            # page_chunk = sorted(list(set([last_chunk_page + 1, n_pages])))\n",
    "            page_chunk_lst.append(page_chunk)\n",
    "        return page_chunk_lst, doc_types_lst\n",
    "\n",
    "    def _handle_auto_classify(self, text_lst: List[str]) -> Tuple[List, List]:\n",
    "        \"\"\"Handling split when upload doctype is auto classify\"\"\"\n",
    "        \n",
    "        system_prompt, user_prompt = self.build_prompts(\n",
    "            text_lst, self.request_config.auto_classification_prompt, is_auto_classify=True, doc_types=self.doc_types\n",
    "        )\n",
    "\n",
    "        output, input_tokens, output_tokens = self.call_llm(LLMClassifier.LLM_PROVIDER, user_prompt, system_prompt)\n",
    "        if not output:\n",
    "            logger.info(f\"[INFO] [Handle AutoClassify] LLM output is empty || llm_output: {output}\")\n",
    "            return [], []\n",
    "        \n",
    "        classification_output = [(el.get(\"label\"), el.get(\"page\")) for el in output.get(\"page_classifications\")]\n",
    "        logger.info(f\"[INFO] Classification Output: {classification_output}\")\n",
    "        pg_chunk_lst, doc_types_lst = self.create_chunk(classification_output)\n",
    "        logger.info(f\"[INFO] After create_chunk, Page Chunks: {pg_chunk_lst}, Doc types: {doc_types_lst}\")\n",
    "        return pg_chunk_lst, doc_types_lst, input_tokens, output_tokens\n",
    "\n",
    "    def _handle_custom_doctype(self, text_lst: List[str], product_prompt: str, doc_type: str) -> List[str]:\n",
    "        \"\"\"Handle split when upload doctype is other than auto classify\"\"\"\n",
    "        system_prompt, user_prompt = self.build_prompts(\n",
    "            text_lst, product_prompt, is_auto_classify=False, doc_types=doc_type\n",
    "        )\n",
    "        output = self.call_llm(LLMClassifier.LLM_PROVIDER, user_prompt, system_prompt)\n",
    "        if not output:\n",
    "            logger.info(f\"[INFO] [Handle Custom Doctype] LLM output is empty || llm_output: {output}\")\n",
    "            return []\n",
    "        doc_types_lst = [[el.get(\"label\") for el in output.get(\"page_classifications\")]]\n",
    "        pg_chunk_lst = [[el.get(\"page\") for el in output.get(\"page_classifications\")]]\n",
    "        # logger.info(f\"[INFO] After custom doctype handling, Page Chunks: {pg_chunk_lst}, Doc types: {doc_types_lst}\")\n",
    "        return pg_chunk_lst, doc_types_lst\n",
    "    \n",
    "    def process_doc_type_classification(self, text_lst, single_batch_text, df_lst):\n",
    "        if self.request_config.auto_classify:  # Auto-classification logic\n",
    "            pg_chunk_lst, doc_types_lst, input_tokens_obtained, output_tokens_obtained = self._handle_auto_classify(single_batch_text)\n",
    "            logger.info(f\"In the process doctype classification the total input and output tokens are:{input_tokens_obtained} and {output_tokens_obtained}\")\n",
    "            if not pg_chunk_lst:\n",
    "                return DEFAULT_RES           \n",
    "            return pg_chunk_lst, doc_types_lst, input_tokens_obtained, output_tokens_obtained\n",
    "\n",
    "        else:  # Single doctype logic\n",
    "            doc_type_config = self.request_config.doc_type_details[0]\n",
    "            if not doc_type_config.prompt:\n",
    "                logger.info(f\"[INFO] Prompt not found for {doc_type_config.doc_type_title} || Doctype config: {doc_type_config}\")\n",
    "                return DEFAULT_RES\n",
    "\n",
    "            if len(text_lst) == 1:\n",
    "                logger.info(f\"[INFO] Skipping custom doctype split as it has single page || len(text_lst): {len(text_lst)}\")\n",
    "                return DEFAULT_RES\n",
    "\n",
    "            default_pg_chunk_lst = [[df.page.unique().tolist()[0] + 1 for df in df_lst]]\n",
    "            default_doc_types_lst = [[doc_type_config.doc_type_title] * len(default_pg_chunk_lst)]\n",
    "\n",
    "            llm_pg_chunk_lst, llm_doc_types_lst = self._handle_custom_doctype(\n",
    "                text_lst, doc_type_config.prompt, self.doc_types[0]\n",
    "            )\n",
    "\n",
    "            return llm_pg_chunk_lst or default_pg_chunk_lst, llm_doc_types_lst or default_doc_types_lst\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75080580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMNonBatched(LLMClassifier):\n",
    "     \n",
    "    def __init__(self, request_config: Union[Dict, RequestConfig], model_name:str) -> None:\n",
    "        print(\"i am here\")\n",
    "        # request_config[\"doc_type_details\"] = [\n",
    "        #     DoctypeConfig(**item) for item in request_config[\"doc_type_details\"]\n",
    "        # ]\n",
    "        request_config = RequestConfig(**request_config)\n",
    "        super().__init__(request_config,model_name)  # This calls the parent class's __init__\n",
    "\n",
    "\n",
    "    def _classify(self, df_lst: List[pd.DataFrame]) -> Dict[str, str]:\n",
    "\n",
    "        text_lst = [get_line_text(df) if not df.empty else \"EMPTY PAGE\" for df in df_lst]\n",
    "        start = time.time()\n",
    "        pg_chunk_lst,doc_types_lst, total_input_tokens, total_output_tokens = self.process_doc_type_classification(text_lst,text_lst,df_lst)\n",
    "        elapsed=time.time()-start\n",
    "        logger.info(f\"[INFO] The Total time taken for the non batched data is: {elapsed}\")\n",
    "        # pg_chunk_lst, doc_types_lst = LLMClassifier.process_chunk(pg_chunk_lst, doc_types_lst, merge_autoclassify=False)\n",
    "        # logger.info(f\"[INFO] After process_chunk, Page Chunks: {pg_chunk_lst}, Doc types: {doc_types_lst}\")\n",
    "        return doc_types_lst, pg_chunk_lst, total_input_tokens, total_output_tokens\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53689e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LLMStrictBatched(LLMClassifier):\n",
    "    def __init__(self, request_config: dict, model_name: str) -> None:\n",
    "        # Initialize RequestConfig from dict if needed\n",
    "        cfg = RequestConfig(**request_config) if isinstance(request_config, dict) else request_config\n",
    "        super().__init__(cfg, model_name)\n",
    "\n",
    "    def split_strict_df(self, text_lst: List[str], batch_size: int = 25) -> List[List[str]]:\n",
    "        \"\"\"Split list of texts into strictly sized batches.\"\"\"\n",
    "        return [text_lst[i : i + batch_size] for i in range(0, len(text_lst), batch_size)]\n",
    "\n",
    "    def _classify(self, df_lst: List[pd.DataFrame]) -> Tuple[List[str], List, int, int]:\n",
    "        # 1. Extract text from DataFrames\n",
    "        text_lst = [get_line_text(df) if not df.empty else \"EMPTY PAGE\" for df in df_lst]\n",
    "        logger.info(f\"The total length of obtained text list is {len(df_lst)} \")\n",
    "        # 2. Batch the text\n",
    "        batched_texts = self.split_strict_df(text_lst)\n",
    "        num_batches = len(batched_texts)\n",
    "\n",
    "        # Placeholder for ordered results and token counts\n",
    "        results: List[tuple] = [None] * num_batches\n",
    "        total_input_tokens = 0\n",
    "        total_output_tokens = 0\n",
    "\n",
    "        def run_batch(idx: int, batch: List[str]):\n",
    "            \"\"\"Wrapper to process one batch and tag with its index.\"\"\"\n",
    "            try:\n",
    "                start = time.time()\n",
    "                pg_chunks, doc_types, input_tokens, output_tokens = self.process_doc_type_classification(\n",
    "                    text_lst, batch, df_lst\n",
    "                )\n",
    "                elapsed = time.time() - start\n",
    "                logger.debug(f\"Batch {idx} processed in {elapsed:.2f}s | Input tokens: {input_tokens}, Output tokens: {output_tokens}\")\n",
    "                return idx, pg_chunks, doc_types, input_tokens, output_tokens\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing batch {idx}: {e}\", exc_info=True)\n",
    "                return idx, [], [], 0, 0  # Fallback to zero tokens\n",
    "\n",
    "        # 3. Execute batches in parallel\n",
    "        overall_start = time.time()\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            futures = [executor.submit(run_batch, i, batch) for i, batch in enumerate(batched_texts)]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                idx, pg_chunks, doc_types, input_tokens, output_tokens = future.result()\n",
    "                results[idx] = (pg_chunks, doc_types)\n",
    "                with token_lock:\n",
    "                    total_input_tokens = input_tokens\n",
    "                    total_output_tokens = output_tokens\n",
    "\n",
    "        logger.info(f\"Processed {num_batches} batches in {time.time() - overall_start:.2f}s\")\n",
    "        logger.info(f\"Total input tokens: {total_input_tokens}, Total output tokens: {total_output_tokens}\")\n",
    "\n",
    "        # 4. Flatten results in correct order\n",
    "        all_pg_chunks: List = []\n",
    "        all_doc_types: List = []\n",
    "        for pg_chunks, doc_types in results:\n",
    "            all_pg_chunks.extend(pg_chunks)\n",
    "            all_doc_types.extend(doc_types)\n",
    "\n",
    "        logger.info(f\"After merging chunks: pages={len(all_pg_chunks)}, types={len(all_doc_types)}\")\n",
    "\n",
    "        #Returning tokens along with results\n",
    "        return all_doc_types, all_pg_chunks, total_input_tokens, total_output_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f92407f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMSpreadBatch(LLMClassifier):\n",
    "\n",
    "    def __init__(self, request_config: dict, model_name:str):\n",
    "        request_config = RequestConfig(**request_config)\n",
    "        super().__init__(request_config,model_name)\n",
    "\n",
    "    def split_spread_text(self, text_lst, batch_size=25, step=20):\n",
    "        batched_texts = []\n",
    "\n",
    "        for i in range(0, len(text_lst), step):\n",
    "            end_batch_index = i + batch_size\n",
    "\n",
    "            if(end_batch_index > len(text_lst) - 1):\n",
    "                end_batch_index=len(text_lst)-1\n",
    "                batch = text_lst[i : end_batch_index+1]\n",
    "                batched_texts.append(batch)\n",
    "                break\n",
    "\n",
    "            batch = text_lst[i : end_batch_index]\n",
    "            batched_texts.append(batch)\n",
    "        print(f\"The length of text_list  is:{len(text_lst)}\")    \n",
    "        print(f\"The length of batched text is:{len(batched_texts)}\")\n",
    "        return batched_texts\n",
    "\n",
    "    \n",
    "    def _classify(self, df_lst: List[pd.DataFrame]) -> Tuple[List[str], List, int, int]:\n",
    "        # Step 1: Extract text from all DataFrames\n",
    "        text_lst = [get_line_text(df) if not df.empty else \"EMPTY PAGE\" for df in df_lst]\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        # Step 2: Batch the text list\n",
    "        batched_texts = self.split_spread_text(text_lst)\n",
    "        num_batches = len(batched_texts)\n",
    "\n",
    "        # Placeholder for ordered results and token counters\n",
    "        results: List[tuple] = [None] * num_batches\n",
    "        total_input_tokens = 0\n",
    "        total_output_tokens = 0\n",
    "\n",
    "        def run_batch(idx: int, batch: List[str]):\n",
    "            \"\"\"Wrapper to process one batch and tag with its index.\"\"\"\n",
    "            try:\n",
    "                start = time.time()\n",
    "                pg_chunks, doc_types, input_tokens, output_tokens = self.process_doc_type_classification(\n",
    "                    text_lst, batch, df_lst\n",
    "                )\n",
    "                elapsed = time.time() - start\n",
    "                logger.debug(f\"Batch {idx} processed in {elapsed:.2f}s | Input Tokens: {input_tokens}, Output Tokens: {output_tokens}\")\n",
    "                return idx, pg_chunks, doc_types, input_tokens, output_tokens\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing batch {idx}: {e}\", exc_info=True)\n",
    "                return idx, [], [], 0, 0\n",
    "\n",
    "        # Step 3: Execute batches in parallel\n",
    "        overall_start = time.time()\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            futures = [executor.submit(run_batch, i, batch) for i, batch in enumerate(batched_texts)]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                idx, pg_chunks, doc_types, input_tokens, output_tokens = future.result()\n",
    "                results[idx] = (pg_chunks, doc_types)\n",
    "                with token_lock:\n",
    "                    total_input_tokens = input_tokens\n",
    "                    total_output_tokens = output_tokens\n",
    "\n",
    "        logger.info(f\"Processed {num_batches} batches in {time.time() - overall_start:.2f}s\")\n",
    "        logger.info(f\"Total input tokens: {total_input_tokens}, Total output tokens: {total_output_tokens}\")\n",
    "\n",
    "        # Step 4: Flatten results in correct order\n",
    "        all_pg_chunks: List = []\n",
    "        all_doc_types: List = []\n",
    "        for pg_chunks, doc_types in results:\n",
    "            all_pg_chunks.extend(pg_chunks)\n",
    "            all_doc_types.extend(doc_types)\n",
    "\n",
    "        logger.info(f\"After merging chunks: pages={len(all_pg_chunks)}, types={len(all_doc_types)}\")\n",
    "\n",
    "        #Return all outputs and token counts\n",
    "        return all_doc_types, all_pg_chunks, total_input_tokens, total_output_tokens\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "626239e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_spread_batched:bool = False\n",
    "is_strict_batched:bool = False\n",
    "is_non_batched: bool = True\n",
    "model_name=\"openai/gpt-4.1-mini\"\n",
    "BASE_DIR = os.getcwd()\n",
    "test_folder_path = os.path.join(BASE_DIR, \"test_single\")\n",
    "COST_MAPPING = {\n",
    "        \"meta-llama/llama-3-70b-instruct\": [8.1e-7, 8.1e-7],\n",
    "        \"openai/gpt-4o\": [0.000005, 0.000015],\n",
    "        \"openai/gpt-4-turbo\": [0.00001, 0.00003],\n",
    "        \"anthropic/claude-3-opus\": [0.000015, 0.000075],\n",
    "        \"google/gemini-flash-1.5\": [2.5e-7, 7.5e-7],\n",
    "        \"qwen/qwen-2-72b-instruct\": [5.9e-7, 7.9e-7],\n",
    "        \"anthropic/claude-3.5-sonnet\": [0.000003, 0.000015],\n",
    "        \"openai/gpt-4.1-mini\": [0.000003, 0.000012]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0057920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification_on_file(file_path):\n",
    "    print(f\"\\n[INFO] Processing: {os.path.basename(file_path)}\")\n",
    "    \n",
    "    df_digital, cdf_digital = parse_pdf(file_path)\n",
    "    df_digital_dict = df_digital.to_dict(orient=\"records\")\n",
    "\n",
    "    # Defining your payload template \n",
    "    payload_template = {\n",
    "        \"auto_classify\": True,\n",
    "        \"auto_classification_prompt\": \"Split each document type into separate pages.\",\n",
    "        \"df\": df_digital_dict,  \n",
    "        \"doc_type_details\": [\n",
    "            {\n",
    "                \"doc_type_id\": \"acord25\",\n",
    "                \"doc_type_title\": \"Acord 25\",\n",
    "                \"prompt\": \"Review every field extracted\"\n",
    "            },\n",
    "            {\n",
    "                \"doc_type_id\": \"invoice\",\n",
    "                \"doc_type_title\": \"Invoice\",\n",
    "                \"prompt\": \"Review every field extracted\"\n",
    "            },\n",
    "            {\n",
    "                \"doc_type_id\": \"form1040\",\n",
    "                \"doc_type_title\": \"Form 1040\",\n",
    "                \"prompt\": \"Review every field extracted.\"\n",
    "            },\n",
    "            {\n",
    "                \"doc_type_id\": \"form1040a\",\n",
    "                \"doc_type_title\": \"Form 1040 A\",\n",
    "                \"prompt\": \"Review every field extracted\"\n",
    "            },\n",
    "            {\n",
    "                \"doc_type_id\": \"form1040b\",\n",
    "                \"doc_type_title\": \"Form 1040 B\",\n",
    "                \"prompt\": \"Review every field extracted\"\n",
    "            },\n",
    "            {\n",
    "                \"doc_type_id\": \"w9\",\n",
    "                \"doc_type_title\": \"W9\",\n",
    "                \"prompt\": \"Review every field extracted.\"\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "  \n",
    "    if is_non_batched:\n",
    "        doc_types, chunks, total_input_tokens, total_output_tokens = LLMNonBatched(request_config=payload_template,model_name=model_name).classify(df=df_digital_dict)\n",
    "        results[\"non_batched\"] = [item for sub in doc_types for item in sub]\n",
    "        results[\"total_input_tokens_non_batched\"]= total_input_tokens\n",
    "        results[\"total_output_tokens_non_batched\"]= total_output_tokens\n",
    "        input_rate, output_rate = COST_MAPPING[model_name]\n",
    "        results[\"cost_non_batched\"] = (total_input_tokens * input_rate) + (total_output_tokens * output_rate)\n",
    "\n",
    "    if is_strict_batched:\n",
    "        doc_types, chunks, total_input_tokens, total_output_tokens = LLMStrictBatched(request_config=payload_template, model_name=model_name).classify(df=df_digital_dict)\n",
    "        results[\"strict_batched\"] = [item for sub in doc_types for item in sub]\n",
    "        results[\"total_input_tokens_strict_batched\"]= total_input_tokens\n",
    "        results[\"total_output_tokens_strict_batched\"]= total_output_tokens\n",
    "        input_rate, output_rate = COST_MAPPING[model_name]\n",
    "        results[\"cost_strict_batched\"] = (total_input_tokens * input_rate) + (total_output_tokens * output_rate)\n",
    "\n",
    "    if is_spread_batched:\n",
    "        doc_types, chunks, total_input_tokens, total_output_tokens = LLMSpreadBatch(request_config=payload_template,model_name=model_name).classify(df=df_digital_dict)\n",
    "        results[\"spread_batched\"] = [item for sub in doc_types for item in sub]\n",
    "        results[\"total_input_tokens_spread_batched\"] = total_input_tokens\n",
    "        results[\"total_output_tokens_spread_batched\"] = total_output_tokens\n",
    "        input_rate, output_rate = COST_MAPPING[model_name]\n",
    "        results[\"cost_spread_batched\"] = (total_input_tokens * input_rate) + (total_output_tokens * output_rate)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14313dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_overlap_classifications(doc_type_lst_spread_batched, window_size=25, stride=5):\n",
    "#     # Flatten the entire list of classifications\n",
    "#     step = window_size - stride  # How much to move the window after each batch\n",
    "#     batch_idx = 0\n",
    "#     final_array = []\n",
    "#     i = 0\n",
    "\n",
    "#     while i < len(doc_type_lst_spread_batched):\n",
    "#         # If we've reached the end of the current window\n",
    "#         if i == batch_idx + window_size:\n",
    "#             # Skip the overlap and move to the next batch\n",
    "#             i += stride \n",
    "#             batch_idx += step + 5 \n",
    "#             continue\n",
    "        \n",
    "#         # Otherwise, add the current item to final array\n",
    "#         final_array.append(doc_type_lst_spread_batched[i])\n",
    "#         i += 1\n",
    "\n",
    "#     return final_array\n",
    "\n",
    "\n",
    "\n",
    "def remove_overlap_classifications(doc_type_lst_spread_batched, window_size=25, stride=5):\n",
    "    step = window_size - stride  # Effective jump after overlap\n",
    "    final_array = []\n",
    "    match_results = []\n",
    "    mismatch_results = []\n",
    "    i = 0\n",
    "    batch_num = 0\n",
    "    print(f\"The total doc tyoe spread length is : {len(doc_type_lst_spread_batched)}\")\n",
    "    while i < len(doc_type_lst_spread_batched):\n",
    "        # Define current window range\n",
    "        window_start = i\n",
    "        window_end = i + window_size\n",
    "        # if(window_end > len(doc_type_lst_spread_batched)):\n",
    "        #     window_end=len(doc_type_lst_spread_batched)\n",
    "        current_window = doc_type_lst_spread_batched[window_start:window_end]\n",
    "\n",
    "        # Add non-overlapping part to final array\n",
    "        if i == 0:\n",
    "            final_array.extend(current_window)\n",
    "        else:\n",
    "            final_array.extend(current_window[stride:])  # skip overlap\n",
    "\n",
    "            # Compare overlap between previous and current window\n",
    "            prev_overlap = doc_type_lst_spread_batched[window_start-5 : window_start]\n",
    "            next_overlap = doc_type_lst_spread_batched[window_start : window_start+5]\n",
    "\n",
    "            details = []\n",
    "            for j, (prev, nxt) in enumerate(zip(prev_overlap, next_overlap)):\n",
    "                if prev != nxt:\n",
    "                    abs_index = window_start + j  # This is the page number in the full list\n",
    "                    mismatch_results.append({\n",
    "                        # \"overlap_batch\": batch_num,\n",
    "                        \"page_number\": abs_index,\n",
    "                        # \"prev_value\": prev,\n",
    "                        # \"next_value\": nxt\n",
    "                    })\n",
    "            \n",
    "\n",
    "        # Move to next window\n",
    "        i += window_size\n",
    "        batch_num += 1\n",
    "    print(mismatch_results)\n",
    "\n",
    "    return final_array, mismatch_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0d2829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_overlap_pages(doc_type_lst_spread_batched, window_size=25, stride=5):\n",
    "    # Flatten the entire list of pages\n",
    "\n",
    "    step = window_size - stride  # How much to move the window after each batch\n",
    "    batch_idx = 0\n",
    "    final_array = []\n",
    "    i = 0\n",
    "\n",
    "    while i + window_size <= len(doc_type_lst_spread_batched):  # Ensure we don't go out of bounds\n",
    "        # Add the indices of the pages in this batch (excluding the overlap)\n",
    "        first_batch_indices = list(range(i, i + step))  # Get the indices of non-overlapping part\n",
    "        final_array.extend(first_batch_indices)\n",
    "        \n",
    "        # Move the window forward by the full window size\n",
    "        i += window_size\n",
    "\n",
    "    # Add remaining indices if any\n",
    "    final_array.extend(range(i, len(doc_type_lst_spread_batched)))\n",
    "\n",
    "    return final_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d668ad9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-01 17:05:04.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Processing: merged_output_10.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-01 17:05:04.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.500\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.908\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:04.956\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.077\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:05.968\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.274\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.604\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.633\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.807\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.869\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:06.989\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.075\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.082\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.215\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.820\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.841\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:07.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:08.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:08.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is None\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:08.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocsumo_image_util.parse.pdf.parse\u001b[0m:\u001b[36m_orient_df\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1morientation is 0\u001b[0m\n",
      "/home/paribartan-timalsina/.pyenv/versions/3.12.3/envs/docsumoenv/lib/python3.12/site-packages/docsumo_image_util/parse/pdf/parse.py:203: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat(df_list).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-01 17:05:10.891\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mclassify\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mThe pages in the parsed document is [0, 2, 4, 6, 7, 8, 9, 10, 11, 14, 15, 16, 17, 20, 22, 23, 24, 25, 26, 27, 28, 29, 32, 33, 34, 36, 38, 40, 41, 42, 43, 44, 45, 48, 49, 50, 51, 54, 56, 57, 58, 59, 60, 61, 62, 63, 66, 67, 68, 70, 72, 74, 75, 76, 77, 78, 79, 82, 83, 84, 85, 88, 90, 91, 92, 93, 94, 95, 96, 97, 100, 101, 102, 104, 106, 108, 109, 110, 111, 112, 113, 116, 117, 118, 119, 122, 124, 125, 126, 127, 128, 129, 130, 131, 134, 135, 136, 138, 140, 142, 143, 144, 145, 146, 147, 150, 151, 152, 153, 156, 158, 159, 160, 161, 162, 163, 164, 165, 168, 169, 170, 172, 174]\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:12.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocllm.caller.base\u001b[0m:\u001b[36m_call_partial\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1m[INFO] Using OpenRouterProvider with model openai/gpt-4.1-mini, allow_model: True and params {'temperature': 0.2, 'frequency_penalty': 0, 'presence_penalty': 0}.\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:46.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_handle_auto_classify\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1m[INFO] Classification Output: [('Acord 25', 1), ('Form 1040', 2), ('W9', 3), ('Form 1040 B', 4), ('auto_classify', 5), ('Form 1040', 6), ('Form 1040', 7), ('Form 1040 B', 8), ('Form 1040 B', 9), ('Form 1040 A', 10), ('W9', 11), ('Form 1040', 12), ('Form 1040', 13), ('Form 1040 A', 14), ('Form 1040 A', 15), ('W9', 16), ('Form 1040', 17), ('Form 1040', 18), ('Acord 25', 19), ('auto_classify', 20), ('Form 1040 B', 21), ('Form 1040 B', 22), ('Form 1040', 23), ('Form 1040', 24), ('Acord 25', 25), ('Form 1040 A', 26), ('W9', 27), ('Form 1040 B', 28), ('auto_classify', 29), ('Form 1040', 30), ('Form 1040', 31), ('Form 1040 B', 32), ('Form 1040 B', 33), ('Form 1040 A', 34), ('W9', 35), ('Form 1040', 36), ('Form 1040', 37), ('Form 1040 A', 38), ('Form 1040 A', 39), ('W9', 40), ('Form 1040', 41), ('Form 1040', 42), ('Acord 25', 43), ('auto_classify', 44), ('Form 1040 B', 45), ('Form 1040 B', 46), ('Form 1040', 47), ('Form 1040', 48), ('Acord 25', 49), ('Form 1040 A', 50), ('W9', 51), ('Form 1040 B', 52), ('auto_classify', 53), ('Form 1040', 54), ('Form 1040', 55), ('Form 1040 B', 56), ('Form 1040 B', 57), ('Form 1040 A', 58), ('W9', 59), ('Form 1040', 60), ('Form 1040', 61), ('Form 1040 A', 62), ('Form 1040 A', 63), ('W9', 64), ('Form 1040', 65), ('Form 1040', 66), ('Acord 25', 67), ('auto_classify', 68), ('Form 1040 B', 69), ('Form 1040 B', 70), ('Form 1040', 71), ('Form 1040', 72), ('Acord 25', 73), ('Form 1040 A', 74), ('W9', 75), ('Form 1040 B', 76), ('auto_classify', 77), ('Form 1040', 78), ('Form 1040', 79), ('Form 1040 B', 80), ('Form 1040 B', 81), ('Form 1040 A', 82), ('W9', 83), ('Form 1040', 84), ('Form 1040', 85), ('Form 1040 A', 86), ('Form 1040 A', 87), ('W9', 88), ('Form 1040', 89), ('Form 1040', 90), ('Acord 25', 91), ('auto_classify', 92), ('Form 1040 B', 93), ('Form 1040 B', 94), ('Form 1040', 95), ('Form 1040', 96), ('Acord 25', 97), ('Form 1040 A', 98), ('W9', 99), ('Form 1040 B', 100), ('auto_classify', 101), ('Form 1040', 102), ('Form 1040', 103), ('Form 1040 B', 104), ('Form 1040 B', 105), ('Form 1040 A', 106), ('W9', 107), ('Form 1040', 108), ('Form 1040', 109), ('Form 1040 A', 110), ('Form 1040 A', 111), ('W9', 112), ('Form 1040', 113), ('Form 1040', 114), ('Acord 25', 115), ('auto_classify', 116), ('Form 1040 B', 117), ('Form 1040 B', 118), ('Form 1040', 119), ('Form 1040', 120), ('Acord 25', 121)]\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:46.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_handle_auto_classify\u001b[0m:\u001b[36m160\u001b[0m - \u001b[1m[INFO] After create_chunk, Page Chunks: [[1], [2], [3], [4], [5], [6, 7], [8, 9], [10], [11], [12, 13], [14, 15], [16], [17, 18], [19], [20], [21, 22], [23, 24], [25], [26], [27], [28], [29], [30, 31], [32, 33], [34], [35], [36, 37], [38, 39], [40], [41, 42], [43], [44], [45, 46], [47, 48], [49], [50], [51], [52], [53], [54, 55], [56, 57], [58], [59], [60, 61], [62, 63], [64], [65, 66], [67], [68], [69, 70], [71, 72], [73], [74], [75], [76], [77], [78, 79], [80, 81], [82], [83], [84, 85], [86, 87], [88], [89, 90], [91], [92], [93, 94], [95, 96], [97], [98], [99], [100], [101], [102, 103], [104, 105], [106], [107], [108, 109], [110, 111], [112], [113, 114], [115], [116], [117, 118], [119, 120], [121]], Doc types: [['Acord 25'], ['Form 1040'], ['W9'], ['Form 1040 B'], ['auto_classify'], ['Form 1040', 'Form 1040'], ['Form 1040 B', 'Form 1040 B'], ['Form 1040 A'], ['W9'], ['Form 1040', 'Form 1040'], ['Form 1040 A', 'Form 1040 A'], ['W9'], ['Form 1040', 'Form 1040'], ['Acord 25'], ['auto_classify'], ['Form 1040 B', 'Form 1040 B'], ['Form 1040', 'Form 1040'], ['Acord 25'], ['Form 1040 A'], ['W9'], ['Form 1040 B'], ['auto_classify'], ['Form 1040', 'Form 1040'], ['Form 1040 B', 'Form 1040 B'], ['Form 1040 A'], ['W9'], ['Form 1040', 'Form 1040'], ['Form 1040 A', 'Form 1040 A'], ['W9'], ['Form 1040', 'Form 1040'], ['Acord 25'], ['auto_classify'], ['Form 1040 B', 'Form 1040 B'], ['Form 1040', 'Form 1040'], ['Acord 25'], ['Form 1040 A'], ['W9'], ['Form 1040 B'], ['auto_classify'], ['Form 1040', 'Form 1040'], ['Form 1040 B', 'Form 1040 B'], ['Form 1040 A'], ['W9'], ['Form 1040', 'Form 1040'], ['Form 1040 A', 'Form 1040 A'], ['W9'], ['Form 1040', 'Form 1040'], ['Acord 25'], ['auto_classify'], ['Form 1040 B', 'Form 1040 B'], ['Form 1040', 'Form 1040'], ['Acord 25'], ['Form 1040 A'], ['W9'], ['Form 1040 B'], ['auto_classify'], ['Form 1040', 'Form 1040'], ['Form 1040 B', 'Form 1040 B'], ['Form 1040 A'], ['W9'], ['Form 1040', 'Form 1040'], ['Form 1040 A', 'Form 1040 A'], ['W9'], ['Form 1040', 'Form 1040'], ['Acord 25'], ['auto_classify'], ['Form 1040 B', 'Form 1040 B'], ['Form 1040', 'Form 1040'], ['Acord 25'], ['Form 1040 A'], ['W9'], ['Form 1040 B'], ['auto_classify'], ['Form 1040', 'Form 1040'], ['Form 1040 B', 'Form 1040 B'], ['Form 1040 A'], ['W9'], ['Form 1040', 'Form 1040'], ['Form 1040 A', 'Form 1040 A'], ['W9'], ['Form 1040', 'Form 1040'], ['Acord 25'], ['auto_classify'], ['Form 1040 B', 'Form 1040 B'], ['Form 1040', 'Form 1040'], ['Acord 25']]\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:46.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_doc_type_classification\u001b[0m:\u001b[36m180\u001b[0m - \u001b[1mIn the process doctype classification the total input and output tokens are:335816 and 3906\u001b[0m\n",
      "\u001b[32m2025-05-01 17:05:46.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_classify\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[INFO] The Total time taken for the non batched data is: 33.791675090789795\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Directory setup\n",
    "output_dir = \"./batching_and_splitting\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "cost_log_path = os.path.join(output_dir, \"cost_log.txt\")\n",
    "\n",
    "def log_cost(batch_type, filename, input_tokens, output_tokens, cost):\n",
    "    with open(cost_log_path, \"a\") as log_file:\n",
    "        log_file.write(\n",
    "            f\"{model_name} | {filename} | {batch_type} | \"\n",
    "            f\"Input Tokens: {input_tokens}, Output Tokens: {output_tokens}, \"\n",
    "            f\"Cost: ${cost:.6f}\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "all_results = {}\n",
    "ground_truth_files=[]\n",
    "for filename in os.listdir(test_folder_path):\n",
    "    if filename.endswith(\".txt\") and not filename.endswith(\"_log.txt\"):\n",
    "        full_path = os.path.join(test_folder_path, filename)\n",
    "        txt_basename = os.path.splitext(filename)[0]\n",
    "        ground_truth_files.append(txt_basename)\n",
    "        \n",
    "    if filename.endswith(\".pdf\"):\n",
    "        full_path = os.path.join(test_folder_path, filename)\n",
    "        pdf_basename = os.path.splitext(filename)[0]\n",
    "\n",
    "        try:\n",
    "            result = run_classification_on_file(full_path)\n",
    "            all_results[filename] = result\n",
    "\n",
    "            # Write non-batched results\n",
    "            if is_non_batched:\n",
    "                # print(f\" Non-Batched - Input Tokens: {result['total_input_tokens_non_batched']}, \"\n",
    "                #     f\"Output Tokens: {result['total_output_tokens_non_batched']}, \"\n",
    "                #     f\"Cost: ${result['cost_non_batched']:.6f}\")\n",
    "                # log_cost(\"Non-Batched\", filename, \n",
    "                #         result['total_input_tokens_non_batched'], \n",
    "                #         result['total_output_tokens_non_batched'], \n",
    "                #         result['cost_non_batched'])\n",
    "\n",
    "                non_batched_path = os.path.join(output_dir, f\"{pdf_basename}_non_batched.txt\")\n",
    "                with open(non_batched_path, \"w\") as file:\n",
    "                    file.write(\"\\n\".join(result[\"non_batched\"]))\n",
    "\n",
    "            # Write strict-batched results\n",
    "            if is_strict_batched:\n",
    "                # print(f\"  Strict-Batched - Input Tokens: {result['total_input_tokens_strict_batched']}, \"\n",
    "                #     f\"Output Tokens: {result['total_output_tokens_strict_batched']}, \"\n",
    "                #     f\"Cost: ${result['cost_strict_batched']:.6f}\")\n",
    "                # log_cost(\"Strict-Batched\", filename, \n",
    "                #         result['total_input_tokens_strict_batched'], \n",
    "                #         result['total_output_tokens_strict_batched'], \n",
    "                #         result['cost_strict_batched'])\n",
    "\n",
    "                strict_batched_path = os.path.join(output_dir, f\"{pdf_basename}_strict_batched.txt\")\n",
    "                with open(strict_batched_path, \"w\") as file:\n",
    "                    file.write(\"\\n\".join(result[\"strict_batched\"]))\n",
    "\n",
    "            # Write spread-batched results (with overlap removed)\n",
    "            if is_spread_batched:\n",
    "                # print(f\"  Spread-Batched - Input Tokens: {result['total_input_tokens_spread_batched']}, \"\n",
    "                #     f\"Output Tokens: {result['total_output_tokens_spread_batched']}, \"\n",
    "                #     f\"Cost: ${result['cost_spread_batched']:.6f}\")\n",
    "                # log_cost(\"Spread-Batched\", filename, \n",
    "                #         result['total_input_tokens_spread_batched'], \n",
    "                #         result['total_output_tokens_spread_batched'], \n",
    "                #         result['cost_spread_batched'])\n",
    "\n",
    "                spread_cleaned, unmatching_result = remove_overlap_classifications(result[\"spread_batched\"])\n",
    "\n",
    "                all_results[filename][\"spread_batched\"] = spread_cleaned\n",
    "                all_results[filename][\"unmatching_result\"] = unmatching_result\n",
    "                spread_batched_path = os.path.join(output_dir, f\"{pdf_basename}_spread_batched.txt\")\n",
    "                with open(spread_batched_path, \"w\") as file:\n",
    "                    file.write(\"\\n\".join(spread_cleaned))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed on {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77274c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(\"test.txt\", \"w\") as file:\n",
    "#     file.write(\"\\n\".join(all_results[\"abc_merged.pdf\"][\"spread_batched\"]))\n",
    "\n",
    "\n",
    "all_results[\"new_merged.pdf\"][\"matching_result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "29f1bc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: new_merged.txt\n",
      "Missing prediction file for non_batched: new_merged_non_batched.txt\n",
      "Missing prediction file for strict_batched: new_merged_strict_batched.txt\n",
      "\n",
      "Classification Report for: spread_batched\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Acord 25       1.00      1.00      1.00        23\n",
      "   Form 1040       1.00      1.00      1.00        50\n",
      "     Invoice       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00       100\n",
      "   macro avg       1.00      1.00      1.00       100\n",
      "weighted avg       1.00      1.00      1.00       100\n",
      "\n",
      "\n",
      "Evaluating: abc_merged.txt\n",
      "Missing prediction file for non_batched: abc_merged_non_batched.txt\n",
      "Missing prediction file for strict_batched: abc_merged_strict_batched.txt\n",
      "\n",
      "Classification Report for: spread_batched\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Acord 25       1.00      1.00      1.00        20\n",
      "   Form 1040       1.00      1.00      1.00        60\n",
      "     Invoice       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           1.00       105\n",
      "   macro avg       1.00      1.00      1.00       105\n",
      "weighted avg       1.00      1.00      1.00       105\n",
      "\n",
      "\n",
      "Evaluating: wxy_merged.txt\n",
      "Missing prediction file for non_batched: wxy_merged_non_batched.txt\n",
      "Missing prediction file for strict_batched: wxy_merged_strict_batched.txt\n",
      "\n",
      "Classification Report for: spread_batched\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Acord 25       1.00      1.00      1.00        20\n",
      "   Form 1040       1.00      1.00      1.00        58\n",
      "     Invoice       1.00      1.00      1.00        26\n",
      "\n",
      "    accuracy                           1.00       104\n",
      "   macro avg       1.00      1.00      1.00       104\n",
      "weighted avg       1.00      1.00      1.00       104\n",
      "\n",
      "\n",
      "===== OVERALL BATCH-TYPE REPORTS =====\n",
      "\n",
      "\n",
      "No data to generate report for non_batched.\n",
      "\n",
      "No data to generate report for strict_batched.\n",
      "\n",
      "=== Overall Report for spread_batched ===\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Acord 25       1.00      1.00      1.00        63\n",
      "   Form 1040       1.00      1.00      1.00       168\n",
      "     Invoice       1.00      1.00      1.00        78\n",
      "\n",
      "    accuracy                           1.00       309\n",
      "   macro avg       1.00      1.00      1.00       309\n",
      "weighted avg       1.00      1.00      1.00       309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "\n",
    "# Directories\n",
    "predicted_dir = \"./batching_and_splitting\"\n",
    "ground_truth_dir = test_folder_path  # Already defined earlier\n",
    "\n",
    "# Batch types to evaluate\n",
    "batching_modes = [\"non_batched\", \"strict_batched\", \"spread_batched\"]\n",
    "\n",
    "# Collect all ground truth files\n",
    "ground_truth_files = [f for f in os.listdir(ground_truth_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "# Prepare accumulators for full evaluation\n",
    "all_y_true = {mode: [] for mode in batching_modes}\n",
    "all_y_pred = {mode: [] for mode in batching_modes}\n",
    "\n",
    "# Evaluation loop per file\n",
    "for gt_file in ground_truth_files:\n",
    "    gt_basename = os.path.splitext(gt_file)[0]\n",
    "    gt_path = os.path.join(ground_truth_dir, gt_file)\n",
    "\n",
    "    with open(gt_path, \"r\") as f:\n",
    "        y_true = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    print(f\"\\nEvaluating: {gt_file}\")\n",
    "\n",
    "    for batch_type in batching_modes:\n",
    "        pred_filename = f\"{gt_basename}_{batch_type}.txt\"\n",
    "        pred_path = os.path.join(predicted_dir, pred_filename)\n",
    "\n",
    "        if not os.path.exists(pred_path):\n",
    "            print(f\"Missing prediction file for {batch_type}: {pred_filename}\")\n",
    "            continue\n",
    "\n",
    "        with open(pred_path, \"r\") as f:\n",
    "            y_pred = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "        if len(y_pred) != len(y_true):\n",
    "            print(f\"Length mismatch for {batch_type} on {gt_file} | y_true: {len(y_true)}, y_pred: {len(y_pred)}\")\n",
    "            continue\n",
    "\n",
    "        # Append to the all-level list for later summary\n",
    "        all_y_true[batch_type].extend(y_true)\n",
    "        all_y_pred[batch_type].extend(y_pred)\n",
    "\n",
    "        # Individual report\n",
    "        report = classification_report(y_true, y_pred, zero_division=0)\n",
    "        print(f\"\\nClassification Report for: {batch_type}\")\n",
    "        print(report)\n",
    "\n",
    "        # Save per-file report\n",
    "        model_name_clean = model_name.replace(\"/\", \"_\")\n",
    "        report_save_path = os.path.join(predicted_dir, f\"{gt_basename}_{batch_type}_{model_name_clean}_report.txt\")\n",
    "        with open(report_save_path, \"w\") as f:\n",
    "            f.write(report)\n",
    "\n",
    "# --- Overall reports ---\n",
    "print(\"\\n===== OVERALL BATCH-TYPE REPORTS =====\\n\")\n",
    "\n",
    "for batch_type in batching_modes:\n",
    "    if all_y_true[batch_type] and all_y_pred[batch_type]:\n",
    "        overall_report = classification_report(all_y_true[batch_type], all_y_pred[batch_type], zero_division=0)\n",
    "        print(f\"\\n=== Overall Report for {batch_type} ===\\n\")\n",
    "        print(overall_report)\n",
    "\n",
    "        # Save to file\n",
    "        overall_report_path = os.path.join(predicted_dir, f\"overall_{batch_type}_{model_name_clean}_report.txt\")\n",
    "        with open(overall_report_path, \"w\") as f:\n",
    "            f.write(overall_report)\n",
    "    else:\n",
    "        print(f\"\\nNo data to generate report for {batch_type}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc4a45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docsumoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
